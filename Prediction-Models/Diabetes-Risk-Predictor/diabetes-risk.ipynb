{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'IMPORTS TO BE USED IN THE LAB'\"\n",
    "\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "from sklearn import metrics as mt\n",
    "from skimage.io import imshow\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score , auc\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#preprocess\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "#1D CNN Imports\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "from keras.utils import FeatureSpace\n",
    "\n",
    "#dl libraraies\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical\n",
    "import keras as keras\n",
    "from keras.layers import Reshape, Input\n",
    "from keras.layers import RandomFlip, RandomRotation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import average\n",
    "from keras.models import  Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import RandomBrightness, RandomContrast\n",
    "from keras.layers import Dropout, Flatten,Activation, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Average\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import shuffle\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Package to Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# For Embeddings\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 520 entries, 0 to 519\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Age                 520 non-null    int64 \n",
      " 1   Gender              520 non-null    object\n",
      " 2   Polyuria            520 non-null    object\n",
      " 3   Polydipsia          520 non-null    object\n",
      " 4   sudden weight loss  520 non-null    object\n",
      " 5   weakness            520 non-null    object\n",
      " 6   Polyphagia          520 non-null    object\n",
      " 7   Genital thrush      520 non-null    object\n",
      " 8   visual blurring     520 non-null    object\n",
      " 9   Itching             520 non-null    object\n",
      " 10  Irritability        520 non-null    object\n",
      " 11  delayed healing     520 non-null    object\n",
      " 12  partial paresis     520 non-null    object\n",
      " 13  muscle stiffness    520 non-null    object\n",
      " 14  Alopecia            520 non-null    object\n",
      " 15  Obesity             520 non-null    object\n",
      " 16  class               520 non-null    object\n",
      "dtypes: int64(1), object(16)\n",
      "memory usage: 69.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./diabetes_risk_prediction_dataset.csv\")\n",
    "\n",
    "cat_labels = ['Gender','Polyuria','Polydipsia','sudden weight loss','weakness','Polyphagia','Genital thrush','visual blurring','Itching','Irritability','delayed healing','partial paresis','muscle stiffness','Alopecia','Obesity','class']\n",
    "num_labels = ['Age']\n",
    "NUM_CLASSES  = 2\n",
    "my_target_list = ['Positive' , 'Negative']\n",
    "\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HELPER FUNCTIONS\"\"\"\n",
    "labels = [ ]\n",
    "# Function to calculate fbeta_score\n",
    "def fbeta_score(y_true, y_pred, beta=1): #OUR FBETA SCORE IS LESS THAN 1\n",
    "    # Clip predictions\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=1)\n",
    "    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=1)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (actual_positives + K.epsilon())\n",
    "\n",
    "    # Calculate F-beta score\n",
    "    beta_squared = beta**2\n",
    "    fbeta = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n",
    "\n",
    "    return K.mean(fbeta)\n",
    "\n",
    "def summarize_net(net, X_test, y_test, title_text=''):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    yhat = np.argmax(net.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)  # Convert one-hot encoded labels to 1d array\n",
    "    acc = mt.accuracy_score(y_true, yhat)\n",
    "    cm = mt.confusion_matrix(y_true, yhat)\n",
    "    cm = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title_text + ' Accuracy: {:.4f}'.format(acc))\n",
    "\n",
    "\n",
    "def setup_data(dataset , cat_cols):\n",
    "    df_processed = dataset.copy()\n",
    "    \n",
    "    # Initialize LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    for column in cat_cols:\n",
    "        # Use label encoding for each categorical column\n",
    "        df_processed[column] = label_encoder.fit_transform(dataset[column])\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def create_dataset_from_dataframe(df_input):\n",
    "\n",
    "    df = df_input.copy()\n",
    "    labels = df['class']\n",
    "\n",
    "    df = {key: value.values[:,np.newaxis] for key, value in df_input[cat_labels+num_labels].items()}\n",
    "\n",
    "    # create the Dataset here\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    \n",
    "    # now enable batching and prefetching\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we care creeating two different types of models, the data preprocessing will be different for each. To start out, we will just save the csv into a variable called 'df' using Pandas `read_csv()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model: Wide and Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the perks of using a Wide and Deep Network is that we can use cross-product features to improve our model, which implements \"memorization\" into our model by allowing it to identify relationships between some of the variables. To create the Wide and Deep Network, we will be using Keras FeatureSpaces to build out the Wide portion, and the we will be using Keras Model and Layers to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=({'Gender': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Polyuria': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Polydipsia': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'sudden weight loss': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'weakness': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Polyphagia': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Genital thrush': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'visual blurring': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Itching': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Irritability': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'delayed healing': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'partial paresis': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'muscle stiffness': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Alopecia': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Obesity': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'class': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'Age': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node Cast defined at (most recent call last):\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 786, in run\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"<ipython-input-9-d7e5c75fc5b0>\", line 70, in <module>\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/utils/feature_space.py\", line 547, in adapt\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/integer_lookup.py\", line 463, in adapt\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/engine/base_preprocessing_layer.py\", line 258, in adapt\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/engine/base_preprocessing_layer.py\", line 123, in adapt_step\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/index_lookup.py\", line 638, in update_state\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/preprocessing_utils.py\", line 34, in ensure_tensor\n\nCast string to int64 is not supported\n\t [[{{node Cast}}]] [Op:__inference_adapt_step_1711]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 70\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Performing One Hot Encoding\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# ds_train_no_label = ds_train.map(lambda x, _: x)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# feature_space.adapt(ds_train_no_label)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m train_ds_with_no_labels \u001b[38;5;241m=\u001b[39m ds_train_no_label\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mfeature_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds_with_no_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/utils/feature_space.py:547\u001b[0m, in \u001b[0;36mFeatureSpace.adapt\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;66;03m# If the rank is 1, add a dimension\u001b[39;00m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;66;03m# so we can reduce on axis=-1.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;66;03m# Note: if rank was previously 0, it is now 1.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m         feature_dataset \u001b[38;5;241m=\u001b[39m feature_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    545\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    546\u001b[0m         )\n\u001b[0;32m--> 547\u001b[0m     \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_adapted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_encoded_features()  \u001b[38;5;66;03m# Finish building the layer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/integer_lookup.py:463\u001b[0m, in \u001b[0;36mIntegerLookup.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    Calling `adapt()` on an `IntegerLookup` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/engine/base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adapt_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m    260\u001b[0m             context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/lab6env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/lab6env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node Cast defined at (most recent call last):\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 786, in run\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 536, in execute_request\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"<ipython-input-9-d7e5c75fc5b0>\", line 70, in <module>\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/utils/feature_space.py\", line 547, in adapt\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/integer_lookup.py\", line 463, in adapt\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/engine/base_preprocessing_layer.py\", line 258, in adapt\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/engine/base_preprocessing_layer.py\", line 123, in adapt_step\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/index_lookup.py\", line 638, in update_state\n\n  File \"/Users/marcoszertuche/anaconda3/envs/lab6env/lib/python3.9/site-packages/keras/src/layers/preprocessing/preprocessing_utils.py\", line 34, in ensure_tensor\n\nCast string to int64 is not supported\n\t [[{{node Cast}}]] [Op:__inference_adapt_step_1711]"
     ]
    }
   ],
   "source": [
    "df_wd = deepcopy(df)\n",
    "# Correcting Bool Variable Representation\n",
    "mappings_for_target = {'Positive': True, 'Negative': False}\n",
    "df_wd['class'] = df_wd['class'].map(mappings_for_target)\n",
    "\n",
    "\n",
    "# Correcting Categorical Variable Representation\n",
    "column_mappings = {}\n",
    "for column in df_wd.drop(columns=['class']).columns:\n",
    "    column_mapping = {category: index for index, category in enumerate(df_wd[column].unique())}\n",
    "    column_mappings[column] = column_mapping\n",
    "for column, column_mapping in column_mappings.items():\n",
    "    df_wd[column] = df_wd[column].map(column_mapping)\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in shuffle_split.split(df):\n",
    "  df_train= df.iloc[train_index]\n",
    "  df_test= df.iloc[test_index]\n",
    "\n",
    "feature_space= FeatureSpace(\n",
    "    features= {\n",
    "        # \"Age\": FeatureSpace.float_discretized(num_bins=10),\n",
    "        \"Gender\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Polyuria\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Polydipsia\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"sudden weight loss\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"weakness\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Polyphagia\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Genital thrush\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"visual blurring\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Itching\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Irritability\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"delayed healing\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"partial paresis\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"muscle stiffness\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Alopecia\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"Obesity\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "    }, crosses=[\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('Polyuria', 'Polydipsia'),\n",
    "            crossing_dim= 2*2),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('Polyuria', 'sudden_weight_loss'),\n",
    "            crossing_dim= 2*2),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('Polyuria', 'weakness'),\n",
    "            crossing_dim= 2*2),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names= ('Polyuria', 'Polyphagia'),\n",
    "            crossing_dim= 2*2)\n",
    "\n",
    "    ],\n",
    "    output_mode=\"concat\"\n",
    ")\n",
    "\n",
    "# Creating Tensors for train and test datasets\n",
    "categorical_headers= df_wd.drop(columns=['class']).columns\n",
    "\n",
    "ds_train= create_dataset_from_dataframe(df_train)\n",
    "ds_test= create_dataset_from_dataframe(df_test)\n",
    "\n",
    "\n",
    "print(ds_train)\n",
    "# Performing One Hot Encoding\n",
    "# ds_train_no_label = ds_train.map(lambda x, _: x)\n",
    "# feature_space.adapt(ds_train_no_label)\n",
    "\n",
    "train_ds_with_no_labels = ds_train_no_label.map(lambda x: x)\n",
    "feature_space.adapt(train_ds_with_no_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "def setup_embedding_from_crossing(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "\n",
    "    # get the size of the feature\n",
    "    N = feature_space.crossers[col_name].num_bins\n",
    "    x = feature_space.crossers[col_name].output\n",
    "\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "\n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def setup_embedding_from_categorical(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    N = len(feature_space.preprocessors[col_name].get_vocabulary())\n",
    "\n",
    "    # get the output from the feature space, which is input to embedding\n",
    "    x = feature_space.preprocessors[col_name].output\n",
    "\n",
    "    # now use an embedding to deal with integers from feature space\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "\n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "\n",
    "    return x # return the tensor here\n",
    "\n",
    "\n",
    "dict_inputs = feature_space.get_inputs() # need to use unprocessed features here, to gain access to each output\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# for each crossed variable, make an embedding\n",
    "for col in feature_space.crossers.keys():\n",
    "    print()\n",
    "    x = setup_embedding_from_crossing(feature_space, col)\n",
    "\n",
    "    # save these outputs in list to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch1 = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs1 = []\n",
    "\n",
    "\n",
    "\n",
    "# for each categorical variable\n",
    "for col in categorical_headers:\n",
    "\n",
    "    # get the output tensor from ebedding layer\n",
    "    x = setup_embedding_from_categorical(feature_space, col)\n",
    "\n",
    "    # save these outputs in list to concatenate later\n",
    "    all_deep_branch_outputs1.append(x)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch1 = Concatenate(name='embed_concat1')(all_deep_branch_outputs1)\n",
    "deep_branch1 = Dense(units=20,activation='relu', name='deep1_1')(deep_branch1)\n",
    "deep_branch1 = Dense(units=5,activation='relu', name='deep1_2')(deep_branch1)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch1 = Concatenate(name='concat_deep_wide1')([deep_branch1, wide_branch1])\n",
    "final_branch1 = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined1')(final_branch1)\n",
    "\n",
    "training_model1 = keras.Model(inputs=dict_inputs, outputs=final_branch1)\n",
    "training_model1.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use it in model.fit\n",
    "history1 = training_model1.fit(\n",
    "    ds_train, epochs=5, validation_data=ds_test, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of FeatureSpace Crosses\n",
    "\n",
    "After a bit of research into early indicators into diabetes, I chose my FeatureSpace crosses to match what was most commonly stated as an early symptom from the different resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = setup_data(df , cat_labels)\n",
    "\n",
    "df_data = df.drop(columns=['class'], axis = 1)\n",
    "\n",
    "df_target = df['class']\n",
    "\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data, df_target, test_size=0.2,\n",
    "                                                            stratify=df_target, \n",
    "                                                            random_state=42)\n",
    "\n",
    "\n",
    "# print some stats of the data\n",
    "print(\"X_train Shape:\",X_train.shape, \"Label Shape:\", y_train.shape)\n",
    "uniq_classes = np.sum(y_train,axis=0)\n",
    "plt.bar(list(range(NUM_CLASSES)),uniq_classes)\n",
    "plt.xticks(list(range(NUM_CLASSES)), my_target_list, rotation='vertical')\n",
    "plt.ylabel(\"Number of Instances in Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Wide and Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
